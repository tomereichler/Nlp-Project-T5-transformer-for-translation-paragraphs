# -*- coding: utf-8 -*-
"""nlp_eval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tTVaIDjyJ1FPpp3cmLAQdJsrWA6Qlh1v
"""

import string
import evaluate
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,Seq2SeqTrainingArguments,DataCollatorForSeq2Seq,Seq2SeqTrainer
import numpy as np
import pandas as pd
import torch
import random
import spacy
from torch.utils.data import Dataset

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def read_file_german(file_path):
    file_en, file_de = [], []
    with open(file_path, encoding='utf-8') as f:
        cur_str, cur_list = '', []
        for line in f.readlines():
            line = line.strip()
            if line.startswith("Roots") or line == 'German:':
                if len(cur_str) > 0:
                    cur_list.append(cur_str.strip())
                    cur_str = ''
                if line.startswith("Roots"):
                    cur_list = file_en
                else:
                    cur_list = file_de
                continue
            cur_str += line + ' '
    if len(cur_str) > 0:
        cur_list.append(cur_str)
    return file_de

def preprocess_unlables(TEXT_PATH):
  with open(TEXT_PATH, 'r', encoding='utf-8') as f:
            paragraphs = f.read()
  paragraphs = paragraphs.split('\n\n')
  df = pd.DataFrame(columns=['id', 'German', 'German_input', 'Roots in English','Modifiers in English'])
  counter = 0
  # Iterate over the paragraphs and extract the German and English sentences
  for p in paragraphs:
      sentences = p.split('\n')
      if len(sentences)>=3:
        german = "".join(sentences[1:len(sentences)-2])
        english_Roots = sentences[len(sentences)-2].split(":")[1].strip().split(", ")
        english_Modifiers = list(sentences[len(sentences)-1].split(":")[1].strip().strip().split(", "))
        result = []
        for i in range(0, len(english_Modifiers), 2):
            if i+1 < len(english_Modifiers):
                x = english_Modifiers[i].strip('()')
                y = english_Modifiers[i+1].strip('()')
                result.append((x, y))
        english_Modifiers = result

        string_list_mod = [str(t) for t in english_Modifiers]
        string_list_roots = [str(t) for t in english_Roots]

        english_Modifiers_for_prefix = "".join(list(string_list_mod))
        english_roots_for_prefix = "".join(list(string_list_roots))
        German_input =  "translate <modifiers -" + english_Modifiers_for_prefix + ',roots - '+ english_roots_for_prefix + '>:' + german
        df = df.append({'id': counter,'German': german,'German_input': German_input, 'Roots in English': english_Roots, 'Modifiers in English': english_Modifiers}, ignore_index=True)
        counter += 1
  return df

def generate_translations(unlabaled_path): #Inference
  testdf = preprocess_unlables(unlabaled_path)
  test_germaninput_list = list(testdf.German_input)
  test_german_list = read_file_german(unlabaled_path)

  tokenizer = AutoTokenizer.from_pretrained("best_model")
  model = AutoModelForSeq2SeqLM.from_pretrained("best_model").to(device)
  with open(unlabaled_path+"_208743658_318736501", 'w', encoding='utf-8') as f:
      for i, sentence in enumerate(test_germaninput_list) :
          tokenized_input = tokenizer.encode_plus(sentence, max_length= 256, pad_to_max_length=True,return_tensors='pt').to(device)
          input_ids = tokenized_input['input_ids']
          attention_mask = tokenized_input['attention_mask']
          outputs = model.generate(input_ids=input_ids,max_new_tokens=1000, attention_mask=attention_mask, num_beams = 8,early_stopping =True)
          prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
          f.write("German:\n" + test_german_list[i] + "\n" + 'English:\n' +  prediction + "\n" + "\n")

test_path = 'comp'
generate_translations(test_path)