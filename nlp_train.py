# -*- coding: utf-8 -*-
"""nlp_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L-n83wUxMMcdFguy1CXlQr_nLvcba3Y4
"""

import string
import evaluate
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,Seq2SeqTrainingArguments,DataCollatorForSeq2Seq,Seq2SeqTrainer
import numpy as np
import pandas as pd
import torch
import random
import spacy
from torch.utils.data import Dataset

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def preprocess_unlables(TEXT_PATH):
  with open(TEXT_PATH, 'r', encoding='utf-8') as f:
            paragraphs = f.read()
  paragraphs = paragraphs.split('\n\n')
  df = pd.DataFrame(columns=['id', 'German', 'German_input', 'Roots in English','Modifiers in English'])
  counter = 0
  # Iterate over the paragraphs and extract the German and English sentences
  for p in paragraphs:
      sentences = p.split('\n')
      if len(sentences)>=3:
        german = "".join(sentences[1:len(sentences)-2])
        english_Roots = sentences[len(sentences)-2].split(":")[1].strip().split(", ")
        english_Modifiers = list(sentences[len(sentences)-1].split(":")[1].strip().strip().split(", "))
        result = []
        for i in range(0, len(english_Modifiers), 2):
            if i+1 < len(english_Modifiers):
                x = english_Modifiers[i].strip('()')
                y = english_Modifiers[i+1].strip('()')
                result.append((x, y))
        english_Modifiers = result

        string_list_mod = [str(t) for t in english_Modifiers]
        string_list_roots = [str(t) for t in english_Roots]

        english_Modifiers_for_prefix = "".join(list(string_list_mod))
        english_roots_for_prefix = "".join(list(string_list_roots))
        German_input =  "translate <modifiers -" + english_Modifiers_for_prefix + ',roots - '+ english_roots_for_prefix + '>:' + german
        df = df.append({'id': counter,'German': german,'German_input': German_input, 'Roots in English': english_Roots, 'Modifiers in English': english_Modifiers}, ignore_index=True)
        counter += 1
  return df

def preprocess_lables(TEXT_PATH):
  with open(TEXT_PATH, 'r', encoding='utf-8') as f:
            paragraphs = f.read()
  paragraphs = paragraphs.split('\n\n')
  df = pd.DataFrame(columns=['id', 'German', 'German_input', 'English','Roots in English','Modifiers in English'])
  # Iterate over the paragraphs and extract the German and English sentences
  counter = 0
  for p in paragraphs:
      sentences = p.split('\n')

      if 'English:' in sentences:
        english_index = sentences.index('English:')
        german = "".join(sentences[1:english_index])
        english = "".join(sentences[english_index+1:])
        english_Roots, english_Modifiers = find_dependencies(english)
        
        string_list_mod = [str(t) for t in english_Modifiers]
        string_list_roots = [str(t) for t in english_Roots]

        english_Modifiers_for_prefix = "".join(list(string_list_mod))
        english_roots_for_prefix = "".join(list(string_list_roots))
        German_input =  "translate <modifiers -" + english_Modifiers_for_prefix + ',roots - '+ english_roots_for_prefix + '>:' + german
        df = df.append({'id': counter,'German': german,'German_input': German_input , 'English': english, 'Roots in English': english_Roots, 'Modifiers in English': english_Modifiers}, ignore_index=True)
        counter += 1
  return df
nlp = spacy.load('en_core_web_sm')
def find_dependencies(sentence):
  roots, modifiers =[], []
  for token in nlp(sentence):
    if token.dep_ == 'ROOT':
      roots.append(token)
      connected = [t.text for t in token.children if t.text not in [char for char in string.punctuation]]         
      try:
        modifiers.append(tuple(random.sample(connected,2)))
      except:
        try:
          modifiers.append(tuple(random.sample(connected,1)))
        except:
          modifiers.append(tuple())
  return roots, modifiers
tarin_df = preprocess_lables("train.labeled")
val_df = preprocess_lables("val.labeled")

from torch.utils.data import Dataset

class My_Dataset(Dataset):

    def __init__(self, df, tokenizer, max_sen_len= 256):
        self.tokenizer = tokenizer
        self.data = df
        self.max_sen_len = max_sen_len
        self.input = self.data.German_input
        self.target = self.data.English
        
    def __len__(self):
        return self.target.size

    def __getitem__(self, idx):
        idx_input = str(self.input[idx])
        idx_target= str(self.target[idx])

        tokenized_input = self.tokenizer.batch_encode_plus([idx_input], max_length= self.max_sen_len, pad_to_max_length=True,return_tensors='pt')
        tokenized_target = self.tokenizer.batch_encode_plus([idx_target], max_length= self.max_sen_len, pad_to_max_length=True,return_tensors='pt')

        input_ids = tokenized_input['input_ids'].squeeze()
        input_mask = tokenized_input['attention_mask'].squeeze()
        target_ids = tokenized_target['input_ids'].squeeze()

        return {'input_ids': input_ids.to(dtype=torch.long), 'attention_mask': input_mask.to(dtype=torch.long), 'labels': target_ids.to(dtype=torch.long)}

model_name = 't5-base'
tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=128, model_max_length=128)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

def compute_metrics(eval_preds):
    metric = evaluate.load("sacrebleu")
    preds, labels = eval_preds
    print(type(preds),type(labels))
    if isinstance(preds, tuple):
        preds = preds[0]
    tagged_en = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    true_en = tokenizer.batch_decode(labels, skip_special_tokens=True)

    tagged_en = [x.strip().lower() for x in tagged_en]
    true_en = [x.strip().lower() for x in true_en]

    result = metric.compute(predictions=tagged_en, references=true_en)
    result = {"bleu": result["score"]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result

#Hyperparamatere
batch_size = 4
train_args = Seq2SeqTrainingArguments(
    f"best_model",
    evaluation_strategy = "epoch",
    save_strategy='epoch',
    learning_rate=1e-4,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size*2,
    weight_decay=0.007,
    save_total_limit=3,
    num_train_epochs=10,
    predict_with_generate=True,
    generation_max_length=200,
    push_to_hub=False,
    greater_is_better=True,
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model,
    train_args,
    train_dataset=My_Dataset(tarin_df,tokenizer),
    eval_dataset=My_Dataset(val_df,tokenizer),
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()
trainer.save_model("best_model")